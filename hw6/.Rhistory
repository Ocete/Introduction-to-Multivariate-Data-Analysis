knitr::opts_chunk$set(echo = TRUE)
library(MASS)
# Com
X <- cbind(c(2,2,2,0,-1,-2,-3),c(2,4,6,0,-4,-4,-4))
colnames(X) <- NULL
n <- dim(X)[1]
r <- dim(X)[2]
# Compute sample mean and S
Ones <- rep(1,n)
x_sample_mean <- 1/n * t(X)%*%Ones
S <- 1/(n-1) * t(X - Ones%*%t(x_sample_mean))%*%(X - Ones%*%t(x_sample_mean))
# eigens
ev <- eigen(S)
eigen_values <- ev$values
V <- ev$vectors
prop <- eigen_values[1]/(S[1,1] + S[2,2])
cat( fractions(prop), '=', eigen_values[1], '/', (S[1,1] + S[2,2]))
S
fractions(eigen_values)
eigen_values
V
V
fractions(V)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
# Com
X <- cbind(c(2,2,2,0,-1,-2,-3),c(2,4,6,0,-4,-4,-4))
colnames(X) <- NULL
n <- dim(X)[1]
r <- dim(X)[2]
# Compute sample mean and S
Ones <- rep(1,n)
x_sample_mean <- 1/n * t(X)%*%Ones
S <- 1/(n-1) * t(X - Ones%*%t(x_sample_mean))%*%(X - Ones%*%t(x_sample_mean))
# eigens
ev <- eigen(S)
eigen_values <- ev$values
V <- ev$vectors
prop <- eigen_values[1]/(S[1,1] + S[2,2])
cat( fractions(prop), '=', eigen_values[1], '/', (S[1,1] + S[2,2]))
# We can see the loadings in V
V
# Compute the correlations
cat('Corr(Y_1, X_1) =', V[1,1]*sqrt(eigen_values[1]/S[1,1]))
cat(' --- Corr(Y_1, X_2) =', V[2,1]*sqrt(eigen_values[1]/S[2,2]))
# Com
D <- cbind(c(1/sqrt(S[1,1]),0),c(0,1/sqrt(S[2,2])))
Z <- (X - Ones%*%t(x_sample_mean))%*%t(D)
colnames(Z) <- NULL
# Compute sample mean and S
z_sample_mean <- 1/n * t(Z)%*%Ones
S_z <- 1/(n-1) * t(Z - Ones%*%t(z_sample_mean))%*%(Z - Ones%*%t(z_sample_mean))
# eigens
ev_z <- eigen(S_z)
eigen_values_z <- ev_z$values
V_z <- ev_z$vectors
prop_z <- eigen_values_z[1]/(S_z[1,1] + S_z[2,2])
cat( fractions(prop_z), '=', eigen_values_z[1], '/', (S_z[1,1] + S_z[2,2]))
# We can see the loadings in V
V_z
# Compute the correlations
cat('Corr(Y_1, X_1) =', V_z[1,1]*sqrt(eigen_values_z[1]))
cat(' --- Corr(Y_1, X_2) =', V_z[2,1]*sqrt(eigen_values_z[1]))
S_z
V_z
eigen_values_z
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
#import data
data <- read.table("T1-5.DAT")
#set up X
X <- as.matrix(data)[,1:7]
colnames(X) <- c('Wind','radiation','CO','NO','N02','0_3','HC')
n <- length(X[,1])
r <- length(X[1,])
# Compute sample mean and S
Ones <- rep(1,n)
x_sample_mean <- 1/n * t(X)%*%Ones
S <- 1/(n-1) * t(X - Ones%*%t(x_sample_mean))%*%(X - Ones%*%t(x_sample_mean))
# eigens
ev <- eigen(S)
eigen_values <- ev$values
V <- ev$vectors
k <- 3
SS <- sum(diag((S)))
LL <- sum(eigen_values[1:k])
cat( fractions(LL/SS), '=', LL, '/', SS)
x_sample_mean
S
diag(s)
diag(S)
eigen_values
V
R
# Com
R <- cor(X)
# eigens
ev_z <- eigen(R)
eigen_values_z <- ev_z$values
V_z <- ev_z$vectors
k <- 3
RR <- sum(diag((R)))
LL_r <- sum(eigen_values_z[1:k])
cat( fractions(LL_r/RR), '=', LL_r, '/', RR, fill=TRUE)
# We can see the loadings in V
V_z
# Com
R <- cor(X)
# eigens
ev_z <- eigen(R)
eigen_values_z <- ev_z$values
V_z <- ev_z$vectors
k <- 3
RR <- sum(diag((R)))
LL_r <- sum(eigen_values_z[1:k])
cat( fractions(LL_r/RR), '=', LL_r, '/', RR, fill=TRUE)
# We can see the loadings in V
eigen_values_z
V_z
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
#import data
data <- read.table("T1-6.DAT")
#set up X
X_1<- as.matrix(data)[1:69,1:5]
X_2<- as.matrix(data)[70:98,1:5]
names <- c('(Age)','(SlL + SIR)','|S1L - S1R|','(S2L + S2R)','|S2L - S2R|')
colnames(X_1)<- names
colnames(X_2) <- names
n1 <- length(X_1[,1])
n2 <- length(X_2[,1])
r <- length(names)
sampleMean<-function(X, n) {
Ones <- rep(1,n)
return (1/n * t(X)%*%Ones)
}
sampleCovariance<-function(X, n, sample_mean) {
Ones <- rep(1,n)
return (1/(n-1) * t(X - Ones%*%t(sample_mean))%*%(X - Ones%*%t(sample_mean)))
}
# If x_0 is NULL, the classifier is printed and nothing else is done
fisher<-function(X_1, X_2, x_0=NULL) {
n1 <- length(X_1[,1])
n2 <- length(X_2[,1])
# Compute sample mean and S
x1_sample_mean <- sampleMean(X_1, n1)
x2_sample_mean <- sampleMean(X_2, n2)
S1 <- sampleCovariance(X_1, n1, x1_sample_mean)
S2 <- sampleCovariance(X_2, n2, x2_sample_mean)
Spooled <- (n1-1)/(n1+n2-2) * S1 + (n2-1)/(n1+n2-2) * S2
# Compute the classification
w <- t(x1_sample_mean - x2_sample_mean) %*% solve(Spooled)
mid <- 1/2*(x1_sample_mean + x2_sample_mean)
# Print our classifier or classify the value
if (is.null(x_0))
cat(w, '*(x_0 -', mid, ') >= 0')
else {
result <- w%*%(x_0 - mid)
if (result >= 0) {
return (1)
}
return (2)
}
}
# Use the classifier with our whole population, without predicting anything
fisher(X_1, X_2)
# Returns 0 if then prediction is correct, 1 otherwise.
classify<-function(X_1, X_2, x_0, expeted_class) {
if ( fisher(X_1, X_2, x_0) != expeted_class ) {
return (1)
}
return (0)
}
# Use the classifier with our whole population, without predicting anything
errors <- 0
for (i in 1:n1) {
errors <- errors + classify(X_1, X_2, X_1[i,], 1)
}
for (i in 1:n2) {
errors <- errors + classify(X_1, X_2, X_2[i,], 2)
}
AER <- errors / (n1+n2)
cat( fractions(AER), '=', errors, '/', (n1+n2))
# Use the classifier with our whole population, without predicting anything
errors <- 0
for (i in 1:n1) {
errors <- errors + classify(X_1[-i,], X_2, X_1[i,], 1)
}
for (i in 1:n2) {
errors <- errors + classify(X_1, X_2[-i,], X_2[i,], 2)
}
EAER <- errors / (n1+n2)
cat( fractions(EAER), '=', errors, '/', (n1+n2))
names
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
# Com
X <- cbind(c(2,2,2,0,-1,-2,-3),c(2,4,6,0,-4,-4,-4))
colnames(X) <- NULL
n <- dim(X)[1]
r <- dim(X)[2]
# Compute sample mean and S
Ones <- rep(1,n)
x_sample_mean <- 1/n * t(X)%*%Ones
S <- 1/(n-1) * t(X - Ones%*%t(x_sample_mean))%*%(X - Ones%*%t(x_sample_mean))
# eigens
ev <- eigen(S)
eigen_values <- ev$values
V <- ev$vectors
prop <- eigen_values[1]/(S[1,1] + S[2,2])
cat( fractions(prop), '=', eigen_values[1], '/', (S[1,1] + S[2,2]))
# We can see the loadings in V
V
# Compute the correlations
cat('Corr(Y_1, X_1) =', V[1,1]*sqrt(eigen_values[1]/S[1,1]))
cat(' --- Corr(Y_1, X_2) =', V[2,1]*sqrt(eigen_values[1]/S[2,2]))
# Com
D <- cbind(c(1/sqrt(S[1,1]),0),c(0,1/sqrt(S[2,2])))
Z <- (X - Ones%*%t(x_sample_mean))%*%t(D)
colnames(Z) <- NULL
# Compute sample mean and S
z_sample_mean <- 1/n * t(Z)%*%Ones
S_z <- 1/(n-1) * t(Z - Ones%*%t(z_sample_mean))%*%(Z - Ones%*%t(z_sample_mean))
# eigens
ev_z <- eigen(S_z)
eigen_values_z <- ev_z$values
V_z <- ev_z$vectors
prop_z <- eigen_values_z[1]/(S_z[1,1] + S_z[2,2])
cat( fractions(prop_z), '=', eigen_values_z[1], '/', (S_z[1,1] + S_z[2,2]))
# We can see the loadings in V
V_z
# Compute the correlations
cat('Corr(Y_1, Z_1) =', V_z[1,1]*sqrt(eigen_values_z[1]))
cat(' --- Corr(Y_1, Z_2) =', V_z[2,1]*sqrt(eigen_values_z[1]))
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
#import data
data <- read.table("T1-6.DAT")
#set up X
X_1<- as.matrix(data)[1:69,1:5]
X_2<- as.matrix(data)[70:98,1:5]
names <- c('(Age)','(SlL + SIR)','|S1L - S1R|','(S2L + S2R)','|S2L - S2R|')
colnames(X_1)<- names
colnames(X_2) <- names
n1 <- length(X_1[,1])
n2 <- length(X_2[,1])
r <- length(names)
sampleMean<-function(X, n) {
Ones <- rep(1,n)
return (1/n * t(X)%*%Ones)
}
sampleCovariance<-function(X, n, sample_mean) {
Ones <- rep(1,n)
return (1/(n-1) * t(X - Ones%*%t(sample_mean))%*%(X - Ones%*%t(sample_mean)))
}
# If x_0 is NULL, the classifier is printed and nothing else is done
fisher<-function(X_1, X_2, x_0=NULL) {
n1 <- length(X_1[,1])
n2 <- length(X_2[,1])
# Compute sample mean and S
x1_sample_mean <- sampleMean(X_1, n1)
x2_sample_mean <- sampleMean(X_2, n2)
S1 <- sampleCovariance(X_1, n1, x1_sample_mean)
S2 <- sampleCovariance(X_2, n2, x2_sample_mean)
Spooled <- (n1-1)/(n1+n2-2) * S1 + (n2-1)/(n1+n2-2) * S2
# Compute the classification
w <- t(x1_sample_mean - x2_sample_mean) %*% solve(Spooled)
mid <- 1/2*(x1_sample_mean + x2_sample_mean)
# Print our classifier or classify the value
if (is.null(x_0))
cat(w, '*(x_0 -', mid, ') >= 0')
else {
result <- w%*%(x_0 - mid)
if (result >= 0) {
return (1)
}
return (2)
}
}
# Use the classifier with our whole population, without predicting anything
fisher(X_1, X_2)
# Returns 0 if then prediction is correct, 1 otherwise.
classify<-function(X_1, X_2, x_0, expeted_class) {
if ( fisher(X_1, X_2, x_0) != expeted_class ) {
return (1)
}
return (0)
}
# Use the classifier with our whole population, without predicting anything
errors <- 0
for (i in 1:n1) {
errors <- errors + classify(X_1, X_2, X_1[i,], 1)
}
for (i in 1:n2) {
errors <- errors + classify(X_1, X_2, X_2[i,], 2)
}
AER <- errors / (n1+n2)
cat( fractions(AER), '=', errors, '/', (n1+n2))
# Use the classifier with our whole population, without predicting anything
errors <- 0
for (i in 1:n1) {
errors <- errors + classify(X_1[-i,], X_2, X_1[i,], 1)
}
for (i in 1:n2) {
errors <- errors + classify(X_1, X_2[-i,], X_2[i,], 2)
}
EAER <- errors / (n1+n2)
cat( fractions(EAER), '=', errors, '/', (n1+n2))
